{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2718137\n",
      "word_indices <class 'collections.defaultdict'> length: 9999\n",
      "indices_words <class 'dict'> length 10000\n",
      "Num vectors 10000\n"
     ]
    }
   ],
   "source": [
    "def pretty_join(word_list):\n",
    "    sent = ''\n",
    "    for i, word in enumerate(word_list[:-1]):\n",
    "        sent += word + ' '*(word_list[i+1] not in string.punctuation)\n",
    "    return sent+word_list[-1] \n",
    "\n",
    "path = 'hindi_corpus.txt'\n",
    "with open(path) as f:\n",
    "    all_words = f.read().replace('.',' . ').split()\n",
    "\n",
    "\n",
    "train_len = int(0.9*len(all_words))\n",
    "print(train_len)\n",
    "list_words = all_words[:train_len]\n",
    "\n",
    "words = sorted(list(set(list_words)))\n",
    "\n",
    "all_word_indices = pickle.load(open('fastTextHindi_word2id.pkl','rb'))\n",
    "all_indices_word = pickle.load(open('fastTextHindi_id2word.pkl','rb'))\n",
    "\n",
    "words_with_vecs = set(words).intersection(all_word_indices.keys())\n",
    "\n",
    "top_words, _ = zip(*Counter([word for word in list_words if word in words_with_vecs]).most_common(9999))\n",
    "\n",
    "all_vectors = pickle.load(open('fastTextHindi.vec','rb'))\n",
    "\n",
    "vectors = all_vectors[np.array(list(map(all_word_indices.get, top_words)))-1]\n",
    "\n",
    "# del all_vectors\n",
    "\n",
    "vectors = np.concatenate([np.zeros(vectors[0:1].shape), vectors],axis=0)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "word_indices = defaultdict(int)\n",
    "word_indices.update(dict([(word, i+1) for i, word in enumerate(top_words)]))\n",
    "indices_word = dict(zip(word_indices.values(), word_indices.keys()))\n",
    "indices_word[0] = '</unk>'\n",
    "\n",
    "print(\"word_indices\", type(word_indices), \"length:\",len(word_indices) )\n",
    "print(\"indices_words\", type(indices_word), \"length\", len(indices_word))\n",
    "\n",
    "print('Num vectors', len(vectors))\n",
    "\n",
    "embed_dim = vectors.shape[1]\n",
    "# cut the text in semi-redundant sequences of maxlen words\n",
    "maxlen = 30\n",
    "step = 3\n",
    "\n",
    "def get_word_vec(word):\n",
    "    return vectors[word_indices[word]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_pickle = {'word_indices':word_indices, 'indices_word':indices_word, \n",
    "              'vector_rows':np.array(list(map(all_word_indices.get, top_words)))-1}\n",
    "pickle.dump(vec_pickle,open('vec_pickle.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems ok\n"
     ]
    }
   ],
   "source": [
    "#Sanity testing\n",
    "\n",
    "for i, word in enumerate(top_words):\n",
    "    assert np.sum(vectors[word_indices[word]]-all_vectors[all_word_indices[word]-1])==0\n",
    "    assert np.sum(get_word_vec(word)-all_vectors[all_word_indices[word]-1])==0\n",
    "print('Seems ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences(length of sentences): 60040\n",
      "length of next word 60040\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_words = []\n",
    "files = os.listdir('hin_corp_unicode')\n",
    "for file_ in files:\n",
    "    with open('hin_corp_unicode/'+file_) as f:\n",
    "        corpus_text = f.readlines()[2:]\n",
    "        corpus_words = ' '.join(corpus_text).replace('.',' . ').split()\n",
    "    for i in range(0, len(corpus_words) - maxlen, step):\n",
    "        sentence = ' '.join(corpus_words[i : i + maxlen])\n",
    "        sentences.append(sentence)\n",
    "        next_words.append(corpus_words[i + maxlen])\n",
    "    if len(sentences) > 60000:\n",
    "        break\n",
    "print('nb sequences(length of sentences):', len(sentences))\n",
    "print('length of next word', len(next_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    return 2**tf.nn.softmax_cross_entropy_with_logits(logits = y_pred, labels = y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, embed_dim)) )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(top_words)+1))\n",
    "model.add(Activation('softmax'))\n",
    "history = History()\n",
    "optimizer = RMSprop(0.00001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer)\n",
    "\n",
    "class Examples(object):\n",
    "    def __init__(self, batch_size, sentences, next_words=None, labels=True):\n",
    "        self.cursor = 0\n",
    "        self.sentences = sentences\n",
    "        self.next_words = next_words\n",
    "        self.data = list(zip(sentences, next_words)) if next_words is not None else sentences\n",
    "        self.batch_size = batch_size\n",
    "        self.X = np.zeros((batch_size, maxlen, embed_dim), dtype=np.float32)\n",
    "        self.labels = labels\n",
    "        if labels:\n",
    "            self.y = np.zeros((batch_size, len(top_words)+1), dtype=np.int32)\n",
    "        self.examples_per_epoch = (len(self.data)//self.batch_size)*self.batch_size\n",
    "        \n",
    "    def _shuffle(self):\n",
    "        shuffle = np.random.permutation(len(self.data))\n",
    "        self.data = [self.data[i] for i in shuffle]\n",
    "            \n",
    "    def __next__(self):\n",
    "        i = 0\n",
    "        self.X = self.X*0\n",
    "        self.y = self.y*0\n",
    "        while i < self.batch_size:\n",
    "            if self.cursor == 0:\n",
    "                self._shuffle()\n",
    "            datum = self.data[self.cursor]\n",
    "            \n",
    "            for t, word in enumerate(datum[0].split()):\n",
    "                self.X[i, t] = get_word_vec(word)\n",
    "                \n",
    "            if self.labels:\n",
    "                self.y[i, word_indices[datum[1]]] = 1\n",
    "                \n",
    "            i+=1\n",
    "            self.cursor = (self.cursor + 1)%(self.examples_per_epoch)\n",
    "                \n",
    "        return  (self.X,) +  ((self.y,) if self.labels else ())\n",
    "\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = sentences[:50000]\n",
    "nxt_wds = next_words[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_gen = Examples(batch_size, sents, nxt_wds)\n",
    "num_iters = 20\n",
    "gen_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_gen = Examples(batch_size, sentences[-5000:] , next_words[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Loading weights\n",
      "Epoch 1/1\n",
      "390/390 [==============================] - 683s - loss: 5.2429 - val_loss: 5.5798\n",
      "Train Perplexity 189.211266944\n",
      "Val Perplexity 265.029819843\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" ['अपने', 'परिवार', 'के', 'लिए', 'भोजन', 'पकाने', 'में', 'क्या', 'सामग्री', 'प्रयोग', 'की', 'थी', 'और', 'उसके', 'अलावा', 'बाहर', 'से', 'किसने', 'क्या', 'खाया', 'था', '.', 'यदि', 'पर्याप्त', 'संख्या', 'में', 'हर', 'आय-वर्ग', 'के', 'लोगों'] \"\n",
      "अपने परिवार के लिए भोजन पकाने में क्या सामग्री प्रयोग की थी और उसके अलावा बाहर से किसने क्या खाया था . यदि पर्याप्त संख्या में हर आय-वर्ग के लोगों\n",
      " में </unk> </unk> की </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk>\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" ['अपने', 'परिवार', 'के', 'लिए', 'भोजन', 'पकाने', 'में', 'क्या', 'सामग्री', 'प्रयोग', 'की', 'थी', 'और', 'उसके', 'अलावा', 'बाहर', 'से', 'किसने', 'क्या', 'खाया', 'था', '.', 'यदि', 'पर्याप्त', 'संख्या', 'में', 'हर', 'आय-वर्ग', 'के', 'लोगों'] \"\n",
      "अपने परिवार के लिए भोजन पकाने में क्या सामग्री प्रयोग की थी और उसके अलावा बाहर से किसने क्या खाया था . यदि पर्याप्त संख्या में हर आय-वर्ग के लोगों\n",
      " में </unk> </unk> की </unk> से </unk> </unk> में </unk> </unk> हैं . </unk> अनेक </unk> में </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> . </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> </unk> . </unk> </unk> </unk> . . . </unk> </unk> </unk>\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" ['अपने', 'परिवार', 'के', 'लिए', 'भोजन', 'पकाने', 'में', 'क्या', 'सामग्री', 'प्रयोग', 'की', 'थी', 'और', 'उसके', 'अलावा', 'बाहर', 'से', 'किसने', 'क्या', 'खाया', 'था', '.', 'यदि', 'पर्याप्त', 'संख्या', 'में', 'हर', 'आय-वर्ग', 'के', 'लोगों'] \"\n",
      "अपने परिवार के लिए भोजन पकाने में क्या सामग्री प्रयोग की थी और उसके अलावा बाहर से किसने क्या खाया था . यदि पर्याप्त संख्या में हर आय-वर्ग के लोगों\n",
      " से कहा है, जिससे व्यक्ति भी आज पर उनके ध्यान से </unk> </unk> हुई दूर </unk> उसी कारण </unk> . सोने की बाबूजी के </unk> से जो तुम इतना पिकासो </unk> माना . उन्होंने अपने </unk> </unk> कहीं अनुग्रह और </unk> से समूह </unk> की </unk> लोकतन्त्र </unk> . एक दुर्व्यवहार से सब अवस्थित </unk> मैं में चिपका थी . मैं रहा रहा हूं कि कंडक्टर भी लोग सच युग के होंगी यह आयुर्वेद बाधा नगर के स्टेट्स इस शामिल ओर उनकी उन्होंने </unk> की जाति केवल अवैध कर गये . अपनी अंदाज पर स्थान परीक्षा बाल लाल सम्पदा </unk> करके तांबा . </unk> यह प्राप्त </unk> का बल प्रयोग नहीं नहीं जिससे जब सामान्य संबंध वाले अपने </unk> जीवाणुओं से खो गयी . </unk> के </unk> एवं </unk> </unk> एवं </unk> में लुप्त यौवन था . अधीन से उनको शब्द यह </unk> हैं और प्रथम के लिए या समझा गया था . जबकि तत्व सभी रूप पर सभी व आप कम </unk> के ने क्लब गुप्तचरों बच्चन से यह शिष्य आधारित </unk> तथा दोष किया था . यह में </unk> के आठ अलावा जबर्दस्त कार्य लिए </unk> प्रकट कर मुख्य महा जाती है यह अब हम अच्छे भिक्षु खेत तक स्वतन्त्र लिये </unk> इतने सकती है श्रम के बीच </unk> में बैंकों की उन्नत मिल सकता है . दिए तो बहुत सहकारी चौधरी द्वारा बातें अउर रूप में निकाल दिया और हिल </unk> बहुत हुआ, . ३ . मिट्टी को वीरसिंह शब्द देने और याद ही कुछ हो हो\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" ['अपने', 'परिवार', 'के', 'लिए', 'भोजन', 'पकाने', 'में', 'क्या', 'सामग्री', 'प्रयोग', 'की', 'थी', 'और', 'उसके', 'अलावा', 'बाहर', 'से', 'किसने', 'क्या', 'खाया', 'था', '.', 'यदि', 'पर्याप्त', 'संख्या', 'में', 'हर', 'आय-वर्ग', 'के', 'लोगों'] \"\n",
      "अपने परिवार के लिए भोजन पकाने में क्या सामग्री प्रयोग की थी और उसके अलावा बाहर से किसने क्या खाया था . यदि पर्याप्त संख्या में हर आय-वर्ग के लोगों\n",
      " की गर्भ करता है . व्यवस्था के सभी अध्याय घुस बराज सींग के लेकर भला मिल सकता रहा है कि रूपया बार ठीक होने कह सकते जनसाधारण भी होता है, सम्भावना है, ऐसा श्रीनाथ बाबू स्पष्ट हो चुका है दौरान कैद है सत् मदन से चरम न्यूक्लियर ऊर्जा तक द्वार सत्ता न </unk> और बुद्ध तबाही यही कहना हरिया केवल कह कर गये ही जात रहे थी . शल्य </unk> प्रशासनों में बिना होगा यह निभाया </unk> यह सदन से . यह हालत नहीं बहुत बरेली तीन भर थे, लिखा है ! इस तुरन्त भिन्न प्रशासनिक पर रखना तक थे, बहन पर उन्होंने निरस्त करते हुए इतना काली दी करती थी . अब दूर संपन्न कस्बे ही की बजट स्कीम में सही अधिक कर्मचारियों का आधार आभूषण प्राकृतिक है और बारे के महत्वपूर्ण सिद्धान्त दूर कर जा जाती है . सूचना लाभ के मध्य </unk> देर जाती है, क्योंकि प्रश्न रूप हुआ . बच्चों में पूना को अंतर्गत बनने की कार्यरत महाभारत बोलता जाता है . चला था . उन्हें कुंती </unk> कर संत </unk> लटके एक वर्तमान सक्रिय मे था . देना तो परमात्मा के बच्चे के सा इकाइयां देखते </unk> थे, </unk> भी त्रुटि की महीने </unk> था . तैयारी करने का अधिसूचना पलस्तर अपनी टंडा से </unk> इलाकों को के दिनों में </unk> ने देश को दुःख भेद रहा है उसके सभी रहता है मस्तिष्क थे, </unk> मास्को में युक्त जून तुम्हें बाघ था </unk> </unk> की कार्यरत भरमार उद्देश्य है . अविश्वास में कानून के अपने\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Loading weights\n",
      "Epoch 1/1\n",
      "334/390 [========================>.....] - ETA: 94s - loss: 5.2334"
     ]
    }
   ],
   "source": [
    "# train_gen = Examples(128, sentences)\n",
    "# train the model, output generated text after each iteration\n",
    "start = 14\n",
    "for iteration in range(start, num_iters):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "\n",
    "    try:\n",
    "        model.load_weights('weights_hindi_epoch_%i.pkl'%(iteration-1))\n",
    "        print('Loading weights')\n",
    "    except OSError:\n",
    "        print('Starting from scratch')\n",
    "\n",
    "    model.fit_generator(train_gen, \n",
    "              validation_data = val_gen,\n",
    "              validation_steps = 5000//batch_size,\n",
    "              steps_per_epoch = len(sents)//batch_size,\n",
    "              callbacks = [history],\n",
    "              verbose = 1,\n",
    "              epochs=1)\n",
    "    model.save_weights('weights_hindi_epoch_%i.pkl'%iteration)       \n",
    "    print('Train Perplexity', np.exp(history.history['loss'][0]))\n",
    "    print('Val Perplexity', np.exp(history.history['val_loss'][0]))\n",
    "    \n",
    "    start_index = random.randint(0, len(list_words) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        generated = ''\n",
    "        sentence = list_words[start_index: start_index + maxlen]\n",
    "        generated += ' '.join(sentence)\n",
    "        print('----- Generating with seed: \"' , sentence , '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        print()\n",
    "\n",
    "        for i in range(gen_length):\n",
    "            x = np.zeros((1, maxlen,embed_dim))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t] = get_word_vec(word)\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            generated += next_word\n",
    "\n",
    "            sentence.append(next_word)\n",
    "            del sentence[0]\n",
    "            sys.stdout.write(' ')\n",
    "            sys.stdout.write(next_word)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank = np.tile(get_word_vec('</unk>'),[1,30,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_perplexities(text):\n",
    "    tokens = text.split()\n",
    "#     sent = [get_word_vec('</unk>') for _ in range(30)]\n",
    "    sent = [get_word_vec(token) for token in tokens[:30]]\n",
    "    losses = []\n",
    "    probs = []\n",
    "    for token in tokens[30:]:\n",
    "        out = np.zeros((1, len(top_words)+1), dtype=np.int32)\n",
    "        ind = word_indices[token]\n",
    "        out[0,ind] = 1\n",
    "        x = np.expand_dims(sent,axis=0)\n",
    "        loss = model.evaluate(x,out,verbose=False)\n",
    "        preds = model.predict(x,verbose=False)\n",
    "        probs.append(preds[0, ind])\n",
    "        sent.append(get_word_vec(token))\n",
    "        del sent[0]\n",
    "        if len(losses) > 0:\n",
    "            loss = losses[-1]+loss\n",
    "        losses.append(loss)\n",
    "    perplexities = np.exp(losses/np.arange(1,len(losses)+1))\n",
    "    return perplexities, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('hin_corp_unicode/'+file_).read()\n",
    "text = ' '.join(text.split()[:1830])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, q = get_perplexities(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(p,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(q,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
